# C√¥ng d·ª•ng: Flask API server, nh·∫≠n vƒÉn b·∫£n t·ª´ frontend, tr·∫£ v·ªÅ TOP 5 topics v·ªõi x√°c su·∫•t

from flask import Flask, request, jsonify
from flask_cors import CORS
import pickle
import re
import os
from underthesea import word_tokenize
from pathlib import Path

app = Flask(__name__)
CORS(app)  # Cho ph√©p frontend g·ªçi API

# Load model v√† vectorizer khi kh·ªüi ƒë·ªông
MODEL_PATH = Path('models/topic_classifier.pkl')
VECTORIZER_PATH = Path('models/vectorizer.pkl')

print("üîÑ ƒêang load model...")

try:
    with open(MODEL_PATH, 'rb') as f:
        model = pickle.load(f)
    with open(VECTORIZER_PATH, 'rb') as f:
        vectorizer = pickle.load(f)
    print("‚úÖ Load model th√†nh c√¥ng!")
except Exception as e:
    print(f"‚ùå L·ªói load model: {e}")
    model = None
    vectorizer = None

def clean_text(text):
    """L√†m s·∫°ch vƒÉn b·∫£n"""
    text = text.lower()
    text = re.sub(r'[^\w\s√°√†·∫£√£·∫°ƒÉ·∫Ø·∫±·∫≥·∫µ·∫∑√¢·∫•·∫ß·∫©·∫´·∫≠√©√®·∫ª·∫Ω·∫π√™·∫ø·ªÅ·ªÉ·ªÖ·ªá√≠√¨·ªâƒ©·ªã√≥√≤·ªè√µ·ªç√¥·ªë·ªì·ªï·ªó·ªô∆°·ªõ·ªù·ªü·ª°·ª£√∫√π·ªß≈©·ª•∆∞·ª©·ª´·ª≠·ªØ·ª±√Ω·ª≥·ª∑·ªπ·ªµƒë]', ' ', text)
    text = ' '.join(text.split())
    return text

def tokenize_text(text):
    """T√°ch t·ª´ ti·∫øng Vi·ªát"""
    try:
        tokens = word_tokenize(text, format="text")
        return tokens
    except:
        return text

@app.route('/')
def home():
    """Endpoint ki·ªÉm tra API ho·∫°t ƒë·ªông"""
    return jsonify({
        'status': 'success',
        'message': 'Topic Classification API is running! ',
        'model_loaded': model is not None
    })

@app.route('/predict', methods=['POST'])
def predict():
    """
    API d·ª± ƒëo√°n topic
    
    Input JSON:
    {
        "text": "VƒÉn b·∫£n c·∫ßn ph√¢n lo·∫°i"
    }
    
    Output JSON:
    {
        "status": "success",
        "predictions": [
            {"topic": "Th·ªÉ thao", "probability": 76.55},
            {"topic": "Kinh t·∫ø", "probability": 12.30},
            ... 
        ],
        "top_topic": "Th·ªÉ thao"
    }
    """
    
    # Ki·ªÉm tra model ƒë√£ load ch∆∞a
    if model is None or vectorizer is None:
        return jsonify({
            'status': 'error',
            'message': 'Model ch∆∞a ƒë∆∞·ª£c load.  Vui l√≤ng train model tr∆∞·ªõc!'
        }), 500
    
    # L·∫•y d·ªØ li·ªáu t·ª´ request
    try:
        data = request.get_json()
        text = data.get('text', '').strip()
        
        # Validate input
        if not text:
            return jsonify({
                'status': 'error',
                'message': 'Vui l√≤ng nh·∫≠p vƒÉn b·∫£n!'
            }), 400
        
        if len(text) < 10:
            return jsonify({
                'status': 'error',
                'message': 'VƒÉn b·∫£n qu√° ng·∫Øn!  Vui l√≤ng nh·∫≠p √≠t nh·∫•t 10 k√Ω t·ª±.'
            }), 400
        
    except Exception as e:
        return jsonify({
            'status': 'error',
            'message': f'L·ªói ƒë·ªçc d·ªØ li·ªáu: {str(e)}'
        }), 400
    
    # Ti·ªÅn x·ª≠ l√Ω vƒÉn b·∫£n
    try:
        text_clean = clean_text(text)
        text_tokenized = tokenize_text(text_clean)
        
        # Vectorize
        text_tfidf = vectorizer.transform([text_tokenized])
        
        # D·ª± ƒëo√°n
        prediction = model.predict(text_tfidf)[0]
        probabilities = model.predict_proba(text_tfidf)[0]
        
        # L·∫•y top 5 topics
        top_indices = probabilities.argsort()[-5:][::-1]
        predictions = []
        
        for idx in top_indices:
            predictions.append({
                'topic': model.classes_[idx],
                'probability': round(probabilities[idx] * 100, 2)
            })
        
        # Tr·∫£ v·ªÅ k·∫øt qu·∫£
        return jsonify({
            'status': 'success',
            'text': text[:100] + ('...' if len(text) > 100 else ''),
            'predictions': predictions,
            'top_topic': prediction
        })
        
    except Exception as e:
        return jsonify({
            'status': 'error',
            'message': f'L·ªói x·ª≠ l√Ω: {str(e)}'
        }), 500

@app.route('/health', methods=['GET'])
def health():
    """Endpoint ki·ªÉm tra s·ª©c kh·ªèe c·ªßa API"""
    return jsonify({
        'status': 'healthy',
        'model_loaded': model is not None,
        'vectorizer_loaded': vectorizer is not None
    })

if __name__ == '__main__':
    print("\n" + "="*60)
    print("üöÄ TOPIC CLASSIFICATION API")
    print("="*60)
    print(f"üìç URL: http://localhost:5000")
    print(f"üìç Endpoints:")
    print(f"   - GET  /          : Ki·ªÉm tra API")
    print(f"   - POST /predict   : D·ª± ƒëo√°n topic")
    print(f"   - GET  /health    : Ki·ªÉm tra s·ª©c kh·ªèe")
    print("="*60 + "\n")
    
    # Use Render-provided PORT when deployed; default 5000 for local dev (Song ngu: Dung PORT tu Render, fallback 5000 khi chay local)
    port = int(os.environ.get('PORT', 5000))
    app.run(debug=True, host='0.0.0.0', port=port)